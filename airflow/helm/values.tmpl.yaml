# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.
---
# Source of default values: https://github.com/apache/airflow/blob/main/chart/values.yaml

# Airflow create user job settings
createUserJob:
  # In case you need to disable the helm hooks that create the jobs after install.
  # Disable this if you are using ArgoCD for example
  useHelmHooks: false
  applyCustomEnv: false

# Airflow database migration job settings
migrateDatabaseJob:
  # In case you need to disable the helm hooks that create the jobs after install.
  # Disable this if you are using ArgoCD for example
  useHelmHooks: false
  applyCustomEnv: false
  # To run database migrations with Argo CD automatically, you will need to add the
  # following. This will run database migrations every time there is a Sync event
  # in Argo CD. While it is not ideal to run the migrations on every sync, it is a
  # trade-off that allows them to be run automatically.
  jobAnnotations:
    "argocd.argoproj.io/hook": Sync

images:
  airflow:
    repository: ${airflow_image_repo}
    tag: ${airflow_image_tag}

# Global default settings for Airflow pods
nodeSelector:
  "karpenter.sh/nodepool": "airflow-core-components"

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: "karpenter.sh/capacity-type"
          operator: "In"
          values: ["on-demand"]
        - key: "karpenter.k8s.aws/instance-family"
          operator: "In"
          values: ["r5"]
        - key: "karpenter.k8s.aws/instance-cpu"
          operator: "In"
          values: ["8"]

topologySpreadConstraints:
- maxSkew: 1
  topologyKey: "topology.kubernetes.io/zone"
  whenUnsatisfiable: "ScheduleAnyway"
  labelSelector:
    matchLabels:
      app: airflow # This label should match all Airflow pods
- maxSkew: 1
  topologyKey: "kubernetes.io/hostname"
  whenUnsatisfiable: "ScheduleAnyway"
  labelSelector:
    matchLabels:
      app: airflow

# Add common labels to all objects and pods defined in this chart.
labels:
  app: airflow

scheduler:
  replicas: 3
  nodeSelector:
    "karpenter.sh/nodepool": "airflow-core-components"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "karpenter.sh/capacity-type"
            operator: "In"
            values: ["on-demand"]
          - key: "karpenter.k8s.aws/instance-family"
            operator: "In"
            # values: ["c6i", "c5"] # Choosing compute-optimized instances
            values: ["r5"] # Choosing memory-optimized instance
          - key: "karpenter.k8s.aws/instance-cpu"
            operator: "In"
            values: ["8"]
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: "topology.kubernetes.io/zone"
    whenUnsatisfiable: "ScheduleAnyway"
    labelSelector:
      matchLabels:
        component: scheduler
  - maxSkew: 1
    topologyKey: "kubernetes.io/hostname"
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        component: scheduler
  labels:
    component: scheduler

triggerer:
  keda:
    enabled: true
    minReplicaCount: 1
  nodeSelector:
    "karpenter.sh/nodepool": "airflow-core-components"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "karpenter.sh/capacity-type"
                operator: "In"
                values: [ "on-demand" ]
              - key: "karpenter.k8s.aws/instance-family"
                operator: "In"
                # values: ["c6i", "c5"] # Choosing compute-optimized instances
                values: [ "r5" ] # Choosing memory-optimized instance
              - key: "karpenter.k8s.aws/instance-cpu"
                operator: "In"
                values: [ "8" ] # Scheduler might benefit from higher CPU

postgresql:
  enabled: false

pgbouncer:
  enabled: true
  replicas: 3
  nodeSelector:
    "karpenter.sh/nodepool": "airflow-core-components"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "karpenter.sh/capacity-type"
                operator: "In"
                values: [ "on-demand" ]
              - key: "karpenter.k8s.aws/instance-family"
                operator: "In"
                # values: ["c6i", "c5"] # Choosing compute-optimized instances
                values: [ "r5" ] # Choosing memory-optimized instance
              - key: "karpenter.k8s.aws/instance-cpu"
                operator: "In"
                values: [ "8" ] # Scheduler might benefit from higher CPU

webserverSecretKeySecretName: ${webserver_secret_name}

webserver:
  replicas: 3

  # Issue 404: DISABLE AIRRLOW AUTHENTICATION (https://github.com/unity-sds/unity-sps/issues/404)
  webserverConfig: |-
    ${webserver_config}

  startupProbe:
    timeoutSeconds: 20
    failureThreshold: 60 # Number of tries before giving up (10 minutes with periodSeconds of 10)
    periodSeconds: 10    # How often to perform the probe

  nodeSelector:
    "karpenter.sh/nodepool": "airflow-core-components"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "karpenter.sh/capacity-type"
            operator: "In"
            values: ["on-demand"]
          - key: "karpenter.k8s.aws/instance-family"
            operator: "In"
            # values: ["c6i", "c5"] # Choosing compute-optimized instances
            values: ["r5"] # Choosing memory-optimized instance
          - key: "karpenter.k8s.aws/instance-cpu"
            operator: "In"
            values: ["8"] # Balancing between CPU and memory
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: "topology.kubernetes.io/zone"
    whenUnsatisfiable: "ScheduleAnyway"
    labelSelector:
      matchLabels:
        component: webserver
  - maxSkew: 1
    topologyKey: "kubernetes.io/hostname"
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        component: webserver
  labels:
    component: webserver

workers:
  nodeSelector:
    "karpenter.sh/nodepool": "airflow-celery-workers"
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: "karpenter.sh/capacity-type"
            operator: "In"
            values: ["spot"]
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: "karpenter.k8s.aws/instance-family"
            operator: "In"
            # values: ["c6i", "c5"] # Choosing compute-optimized instances
            values: ["r5"] # Choosing memory-optimized instance
          - key: "karpenter.k8s.aws/instance-cpu"
            operator: "In"
            values: ["8"]
  topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: "topology.kubernetes.io/zone"
    whenUnsatisfiable: "ScheduleAnyway"
    labelSelector:
      matchLabels:
        component: worker
  - maxSkew: 1
    topologyKey: "kubernetes.io/hostname"
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        component: worker
  labels:
    component: worker

  keda:
    enabled: true
    pollingInterval: 1
    # Minimum node available for celery workers
    # Set to 1 to always have a worker node available to run tasks
    minReplicaCount: 0
    maxReplicaCount: 128
    # Specify HPA related options
    # https://github.com/kubernetes/enhancements/blob/master/keps/sig-autoscaling/853-configurable-hpa-scale-velocity/README.md
    advanced:
      horizontalPodAutoscalerConfig:
        behavior:
          scaleUp:
            policies:
            - type: Percent
              value: 900
              periodSeconds: 30
          scaleDown:
            stabilizationWindowSeconds: 300
            policies:
              - type: Percent
                value: 100
                periodSeconds: 5
  serviceAccount:
    annotations:
      eks.amazonaws.com/role-arn: "${airflow_worker_role_arn}"
  extraVolumes:
    - name: workers-volume
      persistentVolumeClaim:
        claimName: ${workers_pvc_name}
  extraVolumeMounts:
    - name: workers-volume
      mountPath: /shared-task-data
      readOnly: false

data:
  metadataSecretName: ${metadata_secret_name}
  resultBackendSecretName: ~

config:
  logging:
    remote_logging: true
    logging_level: "INFO"
    remote_base_log_folder: ${airflow_logs_s3_location}
    remote_log_conn_id: "aws_default"
    encrypt_s3_logs: false
  celery:
    worker_concurrency: 16
  webserver:
    enable_proxy_fix: 'True'

dags:
  persistence:
    # Enable persistent volume for storing dags
    enabled: true
    # the name of an existing PVC to use
    existingClaim: ${dags_pvc_name}

dagProcessor:
  enabled: true
  replicas: 3
  nodeSelector:
    "karpenter.sh/nodepool": "airflow-core-components"
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: "karpenter.sh/capacity-type"
                operator: "In"
                values: [ "on-demand" ]
              - key: "karpenter.k8s.aws/instance-family"
                operator: "In"
                # values: ["c6i", "c5"] # Choosing compute-optimized instances
                values: [ "r5" ] # Choosing memory-optimized instance
              - key: "karpenter.k8s.aws/instance-cpu"
                operator: "In"
                values: [ "8" ] # Scheduler might benefit from higher CPU

env:
  - name: "AIRFLOW_VAR_KUBERNETES_PIPELINE_NAMESPACE"
    value: "${kubernetes_namespace}"
  - name: "AIRFLOW_VAR_UNITY_PROJECT"
    value: "${unity_project}"
  - name: "AIRFLOW_VAR_UNITY_VENUE"
    value: "${unity_venue}"
  - name: "AIRFLOW_VAR_UNITY_CLUSTER_NAME"
    value: "${unity_cluster_name}"
  - name: "AIRFLOW_VAR_KARPENTER_NODE_POOLS"
    value: "${karpenter_node_pools}"
  - name: "AIRFLOW_VAR_ECR_URI"
    value: "${cwl_dag_ecr_uri}"

# https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/security/api.html
extraEnv: |
  - name: AIRFLOW__CORE__DAGS_FOLDER
    value: "/opt/airflow/dags"
  - name: AIRFLOW__CORE__PLUGINS_FOLDER
    value: "/opt/airflow/plugins"
  - name: AIRFLOW__CORE__LAZY_LOAD_PLUGINS
    value: "False"
  - name: AIRFLOW__API__AUTH_BACKENDS
    value: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
  - name: AIRFLOW__CORE__PARALLELISM
    value: "32768"
  - name: AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG
    value: "1024"
  - name: AIRFLOW__CORE__MAX_ACTIVE_TASKS_PER_DAG
    value: "4096"
  - name: AIRFLOW__SCHEDULER__MAX_DAGRUNS_TO_CREATE_PER_LOOP
    value: "256"
  - name: AIRFLOW__SCHEDULER__SCHEDULER_HEARTBEAT_SEC
    value: "1"
  - name: AIRFLOW__KUBERNETES__WORKER_PODS_CREATION_BATCH_SIZE
    value: "16"
  - name: AIRFLOW__WEBSERVER__NAVBAR_COLOR
    value: "${webserver_navbar_color}"
  - name: AIRFLOW__WEBSERVER__INSTANCE_NAME
    value: "Deployment: ${webserver_instance_name}, ${service_area} Version: ${service_area_version}"
  - name: AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL
    value: "10"
  - name: AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL
    value: "5"
  - name: AIRFLOW__CORE__DEFAULT_POOL_TASK_SLOT_COUNT
    value: "1024"
  - name: AIRFLOW__WEBSERVER__EXPOSE_CONFIG
    value: "True"
